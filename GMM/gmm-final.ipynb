{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6757bac9",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-03-21T11:35:36.192640Z",
     "iopub.status.busy": "2024-03-21T11:35:36.192110Z",
     "iopub.status.idle": "2024-03-21T11:35:40.763508Z",
     "shell.execute_reply": "2024-03-21T11:35:40.762564Z"
    },
    "papermill": {
     "duration": 4.582156,
     "end_time": "2024-03-21T11:35:40.766064",
     "exception": false,
     "start_time": "2024-03-21T11:35:36.183908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-2.27.0.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "import librosa.display\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import isdir, join\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Math\n",
    "import numpy as np\n",
    "from scipy.fftpack import fft\n",
    "from scipy import signal\n",
    "from scipy.io import wavfile\n",
    "import librosa\n",
    "import pickle\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import IPython.display as ipd\n",
    "import librosa.display\n",
    "\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "import plotly.graph_objs as go\n",
    "import plotly.tools as tls\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33621002",
   "metadata": {},
   "source": [
    "# Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fcb81f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gmm_list = [32]\n",
    "num_pca = [1]\n",
    "num_training_examples = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c602b13c",
   "metadata": {},
   "source": [
    "# Preprocessing and feature extraction functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dba63856",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T11:35:57.247134Z",
     "iopub.status.busy": "2024-03-21T11:35:57.246709Z",
     "iopub.status.idle": "2024-03-21T11:35:57.256524Z",
     "shell.execute_reply": "2024-03-21T11:35:57.255270Z"
    },
    "papermill": {
     "duration": 0.020819,
     "end_time": "2024-03-21T11:35:57.258945",
     "exception": false,
     "start_time": "2024-03-21T11:35:57.238126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_extractor(sound_path, win_length_ms=25, hop_length_ms=10):\n",
    "    # Load the audio file\n",
    "    signal, sr = librosa.load(sound_path,sr=8000)\n",
    "    # signal,sr = wavfile.read(sound_path)\n",
    "    # Extract MFCCs\n",
    "    win_length_samples = int(sr * win_length_ms / 1000)\n",
    "    hop_length_samples = int(sr * hop_length_ms / 1000)\n",
    "    mfccs = librosa.feature.mfcc(y=signal, sr=sr, n_mfcc=13, hop_length=hop_length_samples, win_length=win_length_samples)\n",
    "    # mfccs = mfcc(signal,samplerate=sr,nfft = 2048,numcep=13,nfilt=13)\n",
    "    \n",
    "    #Extract first MFCCs derivatives\n",
    "    delta_mfccs = librosa.feature.delta(mfccs)\n",
    "    \n",
    "    # Extract second MFCCs derivatives\n",
    "    delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
    "    \n",
    "    # # Concatenate features\n",
    "    mfccs_features = np.concatenate((mfccs, delta_mfccs, delta2_mfccs))\n",
    "    \n",
    "    # Return all features\n",
    "    return mfccs, delta_mfccs, delta2_mfccs, mfccs_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11d83e5d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T11:35:57.274922Z",
     "iopub.status.busy": "2024-03-21T11:35:57.274488Z",
     "iopub.status.idle": "2024-03-21T11:35:57.285185Z",
     "shell.execute_reply": "2024-03-21T11:35:57.283916Z"
    },
    "papermill": {
     "duration": 0.02169,
     "end_time": "2024-03-21T11:35:57.287728",
     "exception": false,
     "start_time": "2024-03-21T11:35:57.266038",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess(path, is_pca=0, num_pca = 2, window_length_ms=20, hop_length_ms=10):\n",
    "    '''\n",
    "    Return the numpy array\n",
    "    '''\n",
    "    # Get the path of the audio file\n",
    "    audio_file = Path(path)\n",
    "    samples,sample_rate = librosa.load(audio_file,sr=8000)\n",
    "    # print(f\"The original samples are {samples.shape} and sample rate is {sample_rate}\")\n",
    "    # Remove silence at start and end\n",
    "    # TODO: Apply VAD\n",
    "    # samples_trimmed, _= librosa.effects.trim(samples, top_db=60)\n",
    "    a, b, c, d = feature_extractor(audio_file,window_length_ms,hop_length_ms)\n",
    "    # tot= np.concatenate((a,b,c,d)).T\n",
    "    tot = d.T\n",
    "          \n",
    "    # Create a DataFrame with column names as MFCC_1, MFCC_2, etc.\n",
    "    columns = [f'MFCC_{i+1}' for i in range(tot.shape[1])]\n",
    "    df = pd.DataFrame(tot, columns=columns)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    #csv_filename = '{path}.csv'\n",
    "    #df.to_csv(csv_filename, index=False)\n",
    "    \n",
    "    if(is_pca==1):\n",
    "        pca = PCA(n_components=num_pca)\n",
    "        components = pca.fit_transform(df)\n",
    "        df = pd.DataFrame(data=components)\n",
    "    df_new = df.to_numpy()\n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5b994032",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T11:35:57.304369Z",
     "iopub.status.busy": "2024-03-21T11:35:57.303979Z",
     "iopub.status.idle": "2024-03-21T11:35:57.314674Z",
     "shell.execute_reply": "2024-03-21T11:35:57.313448Z"
    },
    "papermill": {
     "duration": 0.02329,
     "end_time": "2024-03-21T11:35:57.318113",
     "exception": false,
     "start_time": "2024-03-21T11:35:57.294823",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_folder(folder_path, is_pca=0, num_pca=2, items=50,window_length_ms=20, hop_length_ms=10):\n",
    "    '''\n",
    "    Return a numpy array containing preprocessed data from all .wav files in the specified folder.\n",
    "    '''\n",
    "    # Initialize an empty list to store data from all files\n",
    "    data_list = []\n",
    "    i = 0\n",
    "    # Iterate over all files in the folder\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if(i>items):\n",
    "            break\n",
    "        # Check if the file is a .wav file\n",
    "        if file_name.endswith('.wav'):\n",
    "            # Get the full path of the audio file\n",
    "            audio_file = os.path.join(folder_path, file_name)\n",
    "            # samples, sample_rate = librosa.load(audio_file, sr=16000)\n",
    "            #print(f\"Processing {audio_file}: original samples are {samples.shape} and sample rate is {sample_rate}\")\n",
    "\n",
    "            # Remove silence at start and end\n",
    "            # samples_trimmed, _ = librosa.effects.trim(samples, top_db=60)\n",
    "            a, b, c, d = feature_extractor(audio_file)\n",
    "            # tot = np.concatenate((a, b, c, d)).T\n",
    "            tot = d.T\n",
    "\n",
    "            # Create a DataFrame with column names as MFCC_1, MFCC_2, etc.\n",
    "            columns = [f'MFCC_{i+1}' for i in range(tot.shape[1])]\n",
    "            df = pd.DataFrame(tot, columns=columns)\n",
    "\n",
    "            if is_pca == 1:\n",
    "                pca = PCA(n_components=num_pca)\n",
    "                components = pca.fit_transform(df)\n",
    "                df = pd.DataFrame(data=components)\n",
    "\n",
    "            # Append the DataFrame to the list\n",
    "            data_list.append(df)\n",
    "        i += 1\n",
    "\n",
    "    # Concatenate all DataFrames in the list to create a single DataFrame\n",
    "    concatenated_df = pd.concat(data_list, ignore_index=True)\n",
    "    \n",
    "    # Convert the DataFrame to a numpy array\n",
    "    array_data = concatenated_df.to_numpy()\n",
    "    \n",
    "    return array_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a49c00",
   "metadata": {
    "papermill": {
     "duration": 0.006933,
     "end_time": "2024-03-21T11:35:57.348038",
     "exception": false,
     "start_time": "2024-03-21T11:35:57.341105",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Comparing Custom and SK-Learn implementation of GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3c25c28c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T11:35:57.364948Z",
     "iopub.status.busy": "2024-03-21T11:35:57.364210Z",
     "iopub.status.idle": "2024-03-21T11:35:57.369321Z",
     "shell.execute_reply": "2024-03-21T11:35:57.368371Z"
    },
    "papermill": {
     "duration": 0.016697,
     "end_time": "2024-03-21T11:35:57.371967",
     "exception": false,
     "start_time": "2024-03-21T11:35:57.355270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b27957fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T11:35:57.391593Z",
     "iopub.status.busy": "2024-03-21T11:35:57.390812Z",
     "iopub.status.idle": "2024-03-21T11:35:57.395970Z",
     "shell.execute_reply": "2024-03-21T11:35:57.394783Z"
    },
    "papermill": {
     "duration": 0.017181,
     "end_time": "2024-03-21T11:35:57.398274",
     "exception": false,
     "start_time": "2024-03-21T11:35:57.381093",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_paths = [\"../Dataset/Language-Recognition-VADaudio/Gujrati-Train\",\"../Dataset/Language-Recognition-VADaudio/Tamil-Train\",\"../Dataset/Language-Recognition-VADaudio/Telugu-Train\"]\n",
    "test_paths = [\"../Dataset/Language-Recognition-VADaudio/Gujrati-Test\",\"../Dataset/Language-Recognition-VADaudio/Tamil-Test\",\"../Dataset/Language-Recognition-VADaudio/Telugu-Test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bcb63ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T11:35:57.415520Z",
     "iopub.status.busy": "2024-03-21T11:35:57.414642Z",
     "iopub.status.idle": "2024-03-21T11:35:57.439940Z",
     "shell.execute_reply": "2024-03-21T11:35:57.438994Z"
    },
    "papermill": {
     "duration": 0.036884,
     "end_time": "2024-03-21T11:35:57.442554",
     "exception": false,
     "start_time": "2024-03-21T11:35:57.405670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from scipy.stats import multivariate_normal\n",
    "import numpy as np\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "024ae858",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMMNew:\n",
    "    def __init__(\n",
    "        self, \n",
    "        n_mixtures=120, \n",
    "        max_iter=100, \n",
    "        covar_type='full', \n",
    "    ):\n",
    "        self.n_mixtures = n_mixtures\n",
    "        self.max_iter = max_iter\n",
    "        self.alphas = np.ones(n_mixtures) / n_mixtures\n",
    "        self.means = None\n",
    "        self.covs = None\n",
    "        self.covar_type = covar_type\n",
    "        self.log_likelihood_plot_list = None\n",
    "        \n",
    "    # get aic and bic score\n",
    "    def aic_bic(self, X):\n",
    "        # Get the log-likelihood\n",
    "        log_likelihood = self.get_loglikelihood(X)\n",
    "        \n",
    "        # Calculate the number of parameters in the model\n",
    "        n_params = self.n_mixtures\n",
    "        \n",
    "        # Calculate AIC and BIC\n",
    "        aic = -2 * log_likelihood + 2 * n_params\n",
    "        bic = -2 * log_likelihood + n_params * np.log(len(X))\n",
    "        \n",
    "        return aic, bic\n",
    "    \n",
    "    \n",
    "    # Function to get the log likelihood\n",
    "    def get_loglikelihood(self,X):\n",
    "        return self.get_score(X)\n",
    "    \n",
    "        # Function to basically get log-likelihood data\n",
    "    def get_score(self,X):\n",
    "        resp = np.zeros((len(X), self.n_mixtures))\n",
    "        # find responsibility of each data point towards a Gaussian (wik) probability of a data point i to be in kth Gaussian mixture \n",
    "        for i in range(self.n_mixtures):\n",
    "            resp[:, i] = self.alphas[i] * multivariate_normal(mean=self.means[i], cov=self.covs[i],allow_singular=True).pdf(X)\n",
    "        return np.mean(np.log(resp.sum(axis=1)))\n",
    "    \n",
    "    # M Step for full covariance matrix\n",
    "    def full_covar(self, X, resp):\n",
    "        d = X.shape[1]\n",
    "        new_covs = np.zeros_like(self.covs)\n",
    "        for i in range(self.n_mixtures):\n",
    "            diff = X - self.means[i]\n",
    "            new_covs[i] = np.dot(resp[:, i] * diff.T, diff) / resp[:, i].sum()\n",
    "            # regularization term to keep the covariance matrix positive semi-definite\n",
    "            new_covs[i] += np.eye(d) * 1e-6\n",
    "        return new_covs\n",
    "    \n",
    "    # M step for diagonal covariance matrix\n",
    "    def diag_covar(self, X, resp):\n",
    "        d = X.shape[1]\n",
    "        new_covs = np.zeros((self.n_mixtures, d,d))\n",
    "        for i in range(self.n_mixtures):\n",
    "            wik = resp[:,i]\n",
    "            num = wik.reshape(X.shape[0],1)*np.square(X-self.means[i])\n",
    "            column_sums = np.sum(num,axis=0).reshape(1,d) # result will be (1,d) matrix\n",
    "            column_sums /= np.sum(wik)\n",
    "            np.fill_diagonal(new_covs[i],column_sums+1e-6) # regularisation term\n",
    "        return new_covs\n",
    "    \n",
    "    # E step\n",
    "    def e_step(self, X):\n",
    "        resp = np.zeros((len(X), self.n_mixtures))\n",
    "        # find responsibility of each data point towards a Gaussian\n",
    "        for i in range(self.n_mixtures):\n",
    "            resp[:, i] = self.alphas[i] * multivariate_normal(mean=self.means[i], cov=self.covs[i],allow_singular=True).pdf(X)\n",
    "        # To plot the variation of log_likelihood\n",
    "        self.log_likelihood_plot_list.append(np.mean(np.log(resp.sum(axis=1))))\n",
    "        resp = resp / resp.sum(axis=1).reshape(-1, 1)\n",
    "        return resp\n",
    "    \n",
    "    def m_step(self, X, resp):        \n",
    "        # M step for alphas\n",
    "        new_alphas = resp.mean(axis=0)\n",
    "        \n",
    "        # M step for means\n",
    "        new_means = np.zeros_like(self.means)\n",
    "        for i in range(self.n_mixtures):\n",
    "            new_means[i] = np.multiply(resp[:, i].reshape(-1, 1), X).sum(axis=0) / resp[:, i].sum()\n",
    "        \n",
    "        # M step for covariance matrix according to type chosen\n",
    "        if self.covar_type == 'full':\n",
    "            new_covs = self.full_covar(X, resp)\n",
    "        elif self.covar_type == 'diag':\n",
    "            new_covs = self.diag_covar(X, resp)\n",
    "        return new_alphas, new_means, new_covs\n",
    "    \n",
    "    # Fit algorithm\n",
    "    def fit(self, X):\n",
    "        total_iteration_time = 0\n",
    "        \n",
    "        start_total = time.time()\n",
    "        d = X.shape[1]\n",
    "        last = 0\n",
    "        # To store the log lijkelihood for every iteration\n",
    "        self.log_likelihood_plot_list = []\n",
    "        \n",
    "        # initialize means as to K means result initally\n",
    "        kmeans_model =  KMeans(self.n_mixtures).fit(X)\n",
    "        self.means = kmeans_model.cluster_centers_\n",
    "        \n",
    "        # initialize cov matrix of data point i as sample covariance matrix\n",
    "        self.covs = np.zeros((self.n_mixtures, d, d))\n",
    "        data_labels = kmeans_model.labels_\n",
    "\n",
    "        for i in range(self.n_mixtures):\n",
    "            self.covs[i] = np.cov(X[data_labels == i].T+0.1)\n",
    "        \n",
    "        # EM - algorithm\n",
    "        for epoch in range(self.max_iter):\n",
    "            last = epoch\n",
    "            # for each data point find its responsibility\n",
    "            # towards each gaussian\n",
    "            resp = self.e_step(X)\n",
    "            \n",
    "            # re-estimation of model parameters\n",
    "            alphas, means, covs = self.m_step(X, resp)\n",
    "            \n",
    "            # Print convergence criteria\n",
    "            if (np.abs(self.alphas - alphas) < 1e-4).all() and \\\n",
    "               (np.abs(self.means - means) < 1e-4).all() and \\\n",
    "               (np.abs(self.covs - covs) < 1e-4).all():\n",
    "                print(\"Converged at iteration:\", step)\n",
    "                break\n",
    "                \n",
    "            self.alphas = alphas\n",
    "            self.means = means\n",
    "            self.covs = covs\n",
    "        \n",
    "        self.log_likelihood_plot_list = self.log_likelihood_plot_list[1:]\n",
    "        end_total = time.time()\n",
    "        total_time = end_total - start_total\n",
    "        print(f\"Average time per iteration: {total_time / (self.max_iter):.4f} seconds\")\n",
    "        plt.figure(figsize=(6, 4)) \n",
    "        plt.plot(range(last), self.log_likelihood_plot_list,color='g', linewidth=2)\n",
    "        plt.xlabel('Number of Iteration')\n",
    "        plt.ylabel('Log Likelihood')\n",
    "        plt.title('Variation of Log Likelihood for each iteration')\n",
    "        plt.tight_layout()  \n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "16598553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get f1 score\n",
    "def compute_f1_score(confusion_matrix):\n",
    "    num_classes = len(confusion_matrix)\n",
    "    f1_scores = []\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        true_positive = confusion_matrix[i][i]\n",
    "        false_positive = sum(confusion_matrix[j][i] for j in range(num_classes) if j != i)\n",
    "        false_negative = sum(confusion_matrix[i][j] for j in range(num_classes) if j != i)\n",
    "\n",
    "        precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "        recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        f1_scores.append(f1_score)\n",
    "\n",
    "    return f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e4ab202",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T11:35:57.510466Z",
     "iopub.status.busy": "2024-03-21T11:35:57.510006Z",
     "iopub.status.idle": "2024-03-21T11:35:57.526248Z",
     "shell.execute_reply": "2024-03-21T11:35:57.525033Z"
    },
    "papermill": {
     "duration": 0.027995,
     "end_time": "2024-03-21T11:35:57.528985",
     "exception": false,
     "start_time": "2024-03-21T11:35:57.500990",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pipeline(n_components_gmm, is_pca, num_pca, c_type='full'):\n",
    "    gmms = []\n",
    "    sklearn_gmms = []  # List to store scikit-learn's GMMs\n",
    "    i = 0\n",
    "    # Define class labels\n",
    "    class_labels = ['Gujrati', 'Tamil', 'Telugu']\n",
    "    \n",
    "    for path in train_paths:\n",
    "        i += 1\n",
    "        X = preprocess_folder(path, is_pca, num_pca, items=num_training_examples)\n",
    "        \n",
    "        # Train and save your GMM\n",
    "        gmm = GMMNew(n_components_gmm, 100, c_type)  # Max 100 iterations\n",
    "        gmm.fit(X)\n",
    "        with open(f'gmm{c_type}_{n_components_gmm}_{num_pca}_{i}.pkl', 'wb') as f:\n",
    "            pickle.dump(gmm, f)\n",
    "        gmms.append(gmm)\n",
    "        \n",
    "        # Train scikit-learn's GMM\n",
    "        sklearn_gmm = GaussianMixture(n_components=n_components_gmm, covariance_type=c_type, max_iter=100)\n",
    "        sklearn_gmm.fit(X)\n",
    "        sklearn_gmms.append(sklearn_gmm)\n",
    "        aic, bic = gmm.aic_bic(X)\n",
    "        print(f\"GMM Model for {class_labels[(i-1)%3]} => AIC: {aic}, BIC: {bic}\")\n",
    "        print(f\"{class_labels[(i-1)%3]} is done\")\n",
    "\n",
    "    # Initialize confusion matrices\n",
    "    confusion_matrix_gmm = np.zeros((3, 3))  # For your GMM\n",
    "    confusion_matrix_sklearn = np.zeros((3, 3))  # For scikit-learn's GMM\n",
    "\n",
    "    # Evaluate your GMM and update confusion matrix\n",
    "    for idx, path in enumerate(test_paths):\n",
    "        class_counts_gmm = {0: 0, 1: 0, 2: 0}\n",
    "        class_counts_sklearn = {0: 0, 1: 0, 2: 0}\n",
    "        for root, _, files in os.walk(path):\n",
    "            for file in files:\n",
    "                vector = preprocess(root+'/'+file, is_pca, num_pca)\n",
    "                \n",
    "                # Evaluate your GMM\n",
    "                log_likelihood_gmm = np.zeros(len(gmms)) \n",
    "                for i in range(len(gmms)):\n",
    "                    gmm = gmms[i]  \n",
    "                    log_likelihood_gmm[i] = gmm.get_score(vector)\n",
    "                \n",
    "                winner_gmm = np.argmax(log_likelihood_gmm)\n",
    "                class_counts_gmm[winner_gmm] += 1\n",
    "\n",
    "                # Evaluate scikit-learn's GMM\n",
    "                log_likelihood_sklearn = np.zeros(len(sklearn_gmms))\n",
    "                for i in range(len(sklearn_gmms)):\n",
    "                    gmm = sklearn_gmms[i]\n",
    "                    log_likelihood_sklearn[i] = np.array(gmm.score(vector)).sum()\n",
    "\n",
    "                winner_sklearn = np.argmax(log_likelihood_sklearn)\n",
    "                class_counts_sklearn[winner_sklearn] += 1\n",
    "\n",
    "        # Update confusion matrices\n",
    "        for true_label, count in class_counts_gmm.items():\n",
    "            confusion_matrix_gmm[idx, true_label] = count\n",
    "        for true_label, count in class_counts_sklearn.items():\n",
    "            confusion_matrix_sklearn[idx, true_label] = count\n",
    "\n",
    "\n",
    "    # Plot confusion matrices side by side\n",
    "    plt.figure(figsize=(14, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.heatmap(confusion_matrix_gmm, annot=True, cmap='Blues', fmt='g', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix for GMM implementation')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(confusion_matrix_sklearn, annot=True, cmap='Oranges', fmt='g', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title(\"Confusion Matrix for Scikit learn implementation\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate and print accuracy and F1 scores\n",
    "    accuracy_gmm = np.trace(confusion_matrix_gmm) / np.sum(confusion_matrix_gmm)\n",
    "    accuracy_sklearn = np.trace(confusion_matrix_sklearn) / np.sum(confusion_matrix_sklearn)\n",
    "    \n",
    "    f1_scores_gmm = compute_f1_score(confusion_matrix_gmm)\n",
    "    f1_scores_sklearn = compute_f1_score(confusion_matrix_sklearn)\n",
    "    \n",
    "    print(\"GMM Implementation Metrics:\")\n",
    "    print(f\"Overall Accuracy: {accuracy_gmm*100:.2f}%\")\n",
    "    for idx, label in enumerate(class_labels):\n",
    "        print(f\"F1 Score of {label}: {f1_scores_gmm[idx]}\")\n",
    "\n",
    "    print(\"\\nScikit-learn's GMM Metrics:\")\n",
    "    print(f\"Overall Accuracy: {accuracy_sklearn*100:.2f}%\")\n",
    "    for idx, label in enumerate(class_labels):\n",
    "        print(f\"F1 Score of {label}: {f1_scores_sklearn[idx]}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e5ab1a",
   "metadata": {
    "papermill": {
     "duration": 0.007228,
     "end_time": "2024-03-21T11:35:57.543774",
     "exception": false,
     "start_time": "2024-03-21T11:35:57.536546",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Getting the results and Metrics along with using PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481b6623",
   "metadata": {},
   "source": [
    "# For diagonal covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5317b746",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-03-21T11:35:57.561450Z",
     "iopub.status.busy": "2024-03-21T11:35:57.561020Z",
     "iopub.status.idle": "2024-03-21T19:20:00.487594Z",
     "shell.execute_reply": "2024-03-21T19:20:00.486304Z"
    },
    "papermill": {
     "duration": 27842.957084,
     "end_time": "2024-03-21T19:20:00.508270",
     "exception": false,
     "start_time": "2024-03-21T11:35:57.551186",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Gaussian mixtures:32, Num of Principal Components:1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     is_pca \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_comp\u001b[49m\u001b[43m,\u001b[49m\u001b[43mis_pca\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_pca_cand\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiag\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(n_components_gmm, is_pca, num_pca, c_type)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m train_paths:\n\u001b[0;32m      9\u001b[0m     i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 10\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_pca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_pca\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitems\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m110\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;66;03m# Train and save your GMM\u001b[39;00m\n\u001b[0;32m     13\u001b[0m     gmm \u001b[38;5;241m=\u001b[39m GMMNew(n_components_gmm, \u001b[38;5;241m100\u001b[39m, c_type)  \u001b[38;5;66;03m# Max 100 iterations\u001b[39;00m\n",
      "Input \u001b[1;32mIn [27]\u001b[0m, in \u001b[0;36mpreprocess_folder\u001b[1;34m(folder_path, is_pca, num_pca, items, window_length_ms, hop_length_ms)\u001b[0m\n\u001b[0;32m     15\u001b[0m audio_file \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, file_name)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# samples, sample_rate = librosa.load(audio_file, sr=16000)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m#print(f\"Processing {audio_file}: original samples are {samples.shape} and sample rate is {sample_rate}\")\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Remove silence at start and end\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# samples_trimmed, _ = librosa.effects.trim(samples, top_db=60)\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m a, b, c, d \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_file\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# tot = np.concatenate((a, b, c, d)).T\u001b[39;00m\n\u001b[0;32m     23\u001b[0m tot \u001b[38;5;241m=\u001b[39m d\u001b[38;5;241m.\u001b[39mT\n",
      "Input \u001b[1;32mIn [25]\u001b[0m, in \u001b[0;36mfeature_extractor\u001b[1;34m(sound_path, win_length_ms, hop_length_ms)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeature_extractor\u001b[39m(sound_path, win_length_ms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m, hop_length_ms\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;66;03m# Load the audio file\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m     signal, sr \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43msound_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43msr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# signal,sr = wavfile.read(sound_path)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Extract MFCCs\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     win_length_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(sr \u001b[38;5;241m*\u001b[39m win_length_ms \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m1000\u001b[39m)\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\librosa\\core\\audio.py:192\u001b[0m, in \u001b[0;36mload\u001b[1;34m(path, sr, mono, offset, duration, dtype, res_type)\u001b[0m\n\u001b[0;32m    189\u001b[0m     y \u001b[38;5;241m=\u001b[39m to_mono(y)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mresample\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_sr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr_native\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_sr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mres_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    195\u001b[0m     sr \u001b[38;5;241m=\u001b[39m sr_native\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\librosa\\core\\audio.py:668\u001b[0m, in \u001b[0;36mresample\u001b[1;34m(y, orig_sr, target_sr, res_type, fix, scale, axis, **kwargs)\u001b[0m\n\u001b[0;32m    662\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mapply_along_axis(\n\u001b[0;32m    663\u001b[0m         samplerate\u001b[38;5;241m.\u001b[39mresample, axis\u001b[38;5;241m=\u001b[39maxis, arr\u001b[38;5;241m=\u001b[39my, ratio\u001b[38;5;241m=\u001b[39mratio, converter_type\u001b[38;5;241m=\u001b[39mres_type\n\u001b[0;32m    664\u001b[0m     )\n\u001b[0;32m    665\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m res_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msoxr\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    666\u001b[0m     \u001b[38;5;66;03m# Use numpy to vectorize the resampler along the target axis\u001b[39;00m\n\u001b[0;32m    667\u001b[0m     \u001b[38;5;66;03m# This is because soxr does not support ndim>2 generally.\u001b[39;00m\n\u001b[1;32m--> 668\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_along_axis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43msoxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    671\u001b[0m \u001b[43m        \u001b[49m\u001b[43marr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    672\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morig_sr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    673\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtarget_sr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    674\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquality\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    675\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    676\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    677\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m resampy\u001b[38;5;241m.\u001b[39mresample(y, orig_sr, target_sr, \u001b[38;5;28mfilter\u001b[39m\u001b[38;5;241m=\u001b[39mres_type, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[1;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\numpy\\lib\\shape_base.py:379\u001b[0m, in \u001b[0;36mapply_along_axis\u001b[1;34m(func1d, axis, arr, *args, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    376\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    377\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot apply_along_axis when any iteration dimensions are 0\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    378\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m--> 379\u001b[0m res \u001b[38;5;241m=\u001b[39m asanyarray(func1d(inarr_view[ind0], \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n\u001b[0;32m    381\u001b[0m \u001b[38;5;66;03m# build a buffer for storing evaluations of func1d.\u001b[39;00m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;66;03m# remove the requested axis, and add the new ones on the end.\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;66;03m# laid out so that each write is contiguous.\u001b[39;00m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;66;03m# for a tuple index inds, buff[inds] = func1d(inarr_view[inds])\u001b[39;00m\n\u001b[0;32m    385\u001b[0m buff \u001b[38;5;241m=\u001b[39m zeros(inarr_view\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m res\u001b[38;5;241m.\u001b[39mshape, res\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Python\\Python310\\lib\\site-packages\\soxr\\__init__.py:159\u001b[0m, in \u001b[0;36mresample\u001b[1;34m(x, in_rate, out_rate, quality)\u001b[0m\n\u001b[0;32m    156\u001b[0m x \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mascontiguousarray(x)    \u001b[38;5;66;03m# make array C-contiguous\u001b[39;00m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 159\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcysoxr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcysoxr_divide_proc\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnewaxis\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    160\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39msqueeze(y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "n_comp_list = num_gmm_list\n",
    "num_pca_list = num_pca\n",
    "for num_pca_cand in num_pca_list:\n",
    "    for n_comp in n_comp_list:\n",
    "        print(f\"Number of Gaussian mixtures:{n_comp}, Num of Principal Components:{num_pca_cand}\")\n",
    "        if(num_pca_cand==39):\n",
    "            is_pca = 0\n",
    "        else:\n",
    "            is_pca = 1\n",
    "        pipeline(n_comp,is_pca,num_pca_cand,'diag')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70109ae1",
   "metadata": {},
   "source": [
    "# For Full covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb8c702",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_pca_cand in num_pca_list:\n",
    "    for n_comp in n_comp_list:\n",
    "        print(f\"Number of Gaussian mixtures:{n_comp}, Num of Principal Components:{num_pca_cand}\")\n",
    "        if(num_pca_cand==39):\n",
    "            is_pca = 0\n",
    "        else:\n",
    "            is_pca = 1\n",
    "        pipeline(n_comp,is_pca,num_pca_cand,'full')"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4640507,
     "sourceId": 7901425,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4642876,
     "sourceId": 7904573,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30674,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27869.093764,
   "end_time": "2024-03-21T19:20:02.049647",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-03-21T11:35:32.955883",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
